{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2878a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, TypedDict, Literal\n",
    "import operator\n",
    "\n",
    "# Load API Key (Ensure your .env file has OPENAI_API_KEY)\n",
    "\n",
    "# Initialize the LLM\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-4o-mini\", # or \"gpt-3.5-turbo\" / \"gpt-4\"\n",
    "#     temperature=0\n",
    "# )\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GEMNI_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"GEMNI_API_KEY not found! Please set it in your .env file.\")\n",
    "\n",
    "print(\"API key loaded\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite\", temperature=0.3, api_key=google_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e6cdeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Metric Structure using standard Pydantic\n",
    "class QualityMetrics(BaseModel):\n",
    "    clarity: int = Field(..., description=\"Score from 1-5 indicating how clear and understandable the text is.\")\n",
    "    completeness: int = Field(..., description=\"Score from 1-5 indicating if all parts of the prompt are addressed.\")\n",
    "    accuracy: int = Field(..., description=\"Score from 1-5 indicating factual correctness and precision.\")\n",
    "    critique: str = Field(..., description=\"Specific constructive feedback on how to improve.\")\n",
    "\n",
    "# 2. Define the Agent State\n",
    "class AdaptiveReflectionState(TypedDict):\n",
    "    task: str\n",
    "    draft: str\n",
    "    metrics_history: List[QualityMetrics]  # Tracks history of scores\n",
    "    iterations: int\n",
    "\n",
    "MAX_ITERATIONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fbebbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Node 1: Generator\n",
    "def generator_node(state: AdaptiveReflectionState) -> dict:\n",
    "    task = state[\"task\"]\n",
    "    \n",
    "    # Check if this is a first draft or a refinement\n",
    "    if not state[\"metrics_history\"]:\n",
    "        # First draft\n",
    "        prompt = f\"\"\"Write a response for the following task:\n",
    "        \n",
    "Task: {task}\n",
    "\n",
    "Provide a clear, complete, and accurate answer.\"\"\"\n",
    "        print(f\"\\n Generating initial draft...\")\n",
    "        \n",
    "    else:\n",
    "        # Refinement based on specific scores\n",
    "        latest_metrics = state[\"metrics_history\"][-1]\n",
    "        draft = state[\"draft\"]\n",
    "        prompt = f\"\"\"Improve the draft based on the following critique and scores (aim for 5/5):\n",
    "        \n",
    "Original Task: {task}\n",
    "Current Draft: {draft}\n",
    "\n",
    "Critique: {latest_metrics.critique}\n",
    "Scores - Clarity: {latest_metrics.clarity}/5, Completeness: {latest_metrics.completeness}/5, Accuracy: {latest_metrics.accuracy}/5\n",
    "\n",
    "Refine the text to improve these specific areas.\"\"\"\n",
    "        print(f\"\\nRefining draft (Iteration {state['iterations'] + 1})...\")\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"draft\": response.content, \"iterations\": state[\"iterations\"] + 1}\n",
    "\n",
    "# Node 2: Critic (Structured)\n",
    "def critic_node(state: AdaptiveReflectionState) -> dict:\n",
    "    # Bind the Pydantic model to the LLM for structured output\n",
    "    structured_critic = llm.with_structured_output(QualityMetrics)\n",
    "    \n",
    "    prompt = f\"\"\"Evaluate the following text based on the task.\n",
    "       \n",
    "Task: {state['task']}\n",
    "Text: {state['draft']}\n",
    "\n",
    "Score the text 1-5 on Clarity, Completeness, and Accuracy. Provide constructive feedback.\"\"\"\n",
    "    \n",
    "    print(\" Critiquing...\")\n",
    "    metrics: QualityMetrics = structured_critic.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Print current scores for visibility\n",
    "    print(f\"   Scores -> Clarity: {metrics.clarity}, Completeness: {metrics.completeness}, Accuracy: {metrics.accuracy}\")\n",
    "    \n",
    "    # Append new metrics to history\n",
    "    return {\"metrics_history\": [metrics]}\n",
    "\n",
    "# Logic: Stop only if ALL scores are >= 4 or max iterations reached\n",
    "def should_continue(state: AdaptiveReflectionState) -> Literal[\"generator\", \"end\"]:\n",
    "    latest_metrics = state[\"metrics_history\"][-1]\n",
    "    \n",
    "    # Stop conditions\n",
    "    if state[\"iterations\"] >= MAX_ITERATIONS:\n",
    "        print(\" Max iterations reached.\")\n",
    "        return \"end\"\n",
    "    \n",
    "    # Quality Check: If ALL scores are >= 4, we are done\n",
    "    if (latest_metrics.clarity >= 4 and \n",
    "        latest_metrics.completeness >= 4 and \n",
    "        latest_metrics.accuracy >= 4):\n",
    "        print(\" Quality standards met!\")\n",
    "        return \"end\"\n",
    "    \n",
    "    # Otherwise, refine again\n",
    "    return \"generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c23bcfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AdaptiveReflectionState)\n",
    "\n",
    "builder.add_node(\"generator\", generator_node)\n",
    "builder.add_node(\"critic\", critic_node)\n",
    "\n",
    "builder.add_edge(START, \"generator\")\n",
    "builder.add_edge(\"generator\", \"critic\")\n",
    "builder.add_conditional_edges(\n",
    "    \"critic\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"generator\": \"generator\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "adaptive_agent = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64216b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Adaptive Reflection on: 'explain quantum physics'\n",
      "\n",
      " Generating initial draft...\n",
      " Critiquing...\n",
      "   Scores -> Clarity: 5, Completeness: 5, Accuracy: 5\n",
      " Quality standards met!\n",
      "\n",
      "==================== SCORING HISTORY ====================\n",
      "Iter  | Clarity  | Comp.    | Acc.     | Status\n",
      "-------------------------------------------------------\n",
      "1     | 5        | 5        | 5        | Pass\n",
      "-------------------------------------------------------\n",
      "Final Output:\n",
      "## Quantum Physics: A Journey into the Bizarre and Wonderful World of the Very Small\n",
      "\n",
      "Quantum physic...\n"
     ]
    }
   ],
   "source": [
    "def visualize_history(result):\n",
    "    print(f\"\\n{'='*20} SCORING HISTORY {'='*20}\")\n",
    "    print(f\"{'Iter':<5} | {'Clarity':<8} | {'Comp.':<8} | {'Acc.':<8} | {'Status'}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for i, m in enumerate(result[\"metrics_history\"]):\n",
    "        status = \"Refine\" if (m.clarity < 4 or m.completeness < 4 or m.accuracy < 4) else \"Pass\"\n",
    "        print(f\"{i+1:<5} | {m.clarity:<8} | {m.completeness:<8} | {m.accuracy:<8} | {status}\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"Final Output:\\n{result['draft'][:100]}...\")\n",
    "\n",
    "# Run the test\n",
    "query = \"explain quantum physics\"\n",
    "\n",
    "print(f\"Starting Adaptive Reflection on: '{query}'\")\n",
    "result = adaptive_agent.invoke({\n",
    "    \"task\": query,\n",
    "    \"draft\": \"\",\n",
    "    \"metrics_history\": [],\n",
    "    \"iterations\": 0\n",
    "})\n",
    "\n",
    "visualize_history(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b98e6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Adaptive Reflection on: 'Write a professional business proposal to microsoft requesting a for a branch in Abeokuta'\n",
      "\n",
      " Generating initial draft...\n",
      " Critiquing...\n",
      "   Scores -> Clarity: 5, Completeness: 5, Accuracy: 5\n",
      " Quality standards met!\n",
      "\n",
      "==================== SCORING HISTORY ====================\n",
      "Iter  | Clarity  | Comp.    | Acc.     | Status\n",
      "-------------------------------------------------------\n",
      "1     | 5        | 5        | 5        | Pass\n",
      "-------------------------------------------------------\n",
      "Final Output:\n",
      "## Business Proposal: Establishing a Microsoft Branch in Abeokuta, Nigeria\n",
      "\n",
      "**To:**\n",
      "The Executive Le...\n"
     ]
    }
   ],
   "source": [
    "query = \"Write a professional business proposal to microsoft requesting a for a branch in Abeokuta\"\n",
    "\n",
    "print(f\"Starting Adaptive Reflection on: '{query}'\")\n",
    "result = adaptive_agent.invoke({\n",
    "    \"task\": query,\n",
    "    \"draft\": \"\",\n",
    "    \"metrics_history\": [],\n",
    "    \"iterations\": 1\n",
    "})\n",
    "\n",
    "visualize_history(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
